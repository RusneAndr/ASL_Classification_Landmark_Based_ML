{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00cab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: <class 'sklearn.pipeline.Pipeline'>\n",
      "Loaded label encoder classes: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'space']\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from collections import deque\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "MODEL_PATH = \"model1_best_svm_mediapipe_keypoints.joblib\"\n",
    "LE_PATH = \"model1_label_encoder.joblib\"\n",
    "\n",
    "CAM_INDEX = 0  # try 1 if you have multiple cameras\n",
    "MAX_NUM_HANDS = 1\n",
    "\n",
    "# simple prediction smoothing (helps reduce flicker)\n",
    "SMOOTHING_WINDOW = 7\n",
    "\n",
    "# -----------------------\n",
    "# LOAD MODEL + ENCODER\n",
    "# -----------------------\n",
    "model = joblib.load(MODEL_PATH)\n",
    "le = joblib.load(LE_PATH)\n",
    "\n",
    "print(\"Loaded model:\", type(model))\n",
    "print(\"Loaded label encoder classes:\", list(le.classes_))\n",
    "\n",
    "# -----------------------\n",
    "# MEDIAPIPE INIT\n",
    "# -----------------------\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=MAX_NUM_HANDS,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# HELPERS\n",
    "# -----------------------\n",
    "def landmarks_to_63(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Convert MediaPipe 21 landmarks into a (63,) float32 vector: [x1,y1,z1, x2,y2,z2, ...]\n",
    "    \"\"\"\n",
    "    feats = []\n",
    "    for lm in hand_landmarks.landmark:\n",
    "        feats.extend([lm.x, lm.y, lm.z])\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def predict_letter(x63: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns (label_str, score_float).\n",
    "    Score is based on SVM decision_function margin (not a true probability unless SVC(probability=True)).\n",
    "    \"\"\"\n",
    "    X = x63.reshape(1, -1)\n",
    "    pred_idx = model.predict(X)[0]\n",
    "    label = le.inverse_transform([pred_idx])[0]\n",
    "\n",
    "    # decision_function gives margins; larger = more confident\n",
    "    score = None\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        margins = model.decision_function(X)\n",
    "        # margins shape: (n_classes,) for OvR or (1, n_classes)\n",
    "        margins = np.array(margins).reshape(-1)\n",
    "        score = float(np.max(margins))\n",
    "    return label, score\n",
    "\n",
    "# For smoothing: keep last N labels and majority vote\n",
    "recent = deque(maxlen=SMOOTHING_WINDOW)\n",
    "\n",
    "def majority_vote(labels):\n",
    "    if not labels:\n",
    "        return None\n",
    "    vals, counts = np.unique(labels, return_counts=True)\n",
    "    return vals[np.argmax(counts)]\n",
    "\n",
    "# -----------------------\n",
    "# WEBCAM LOOP\n",
    "# -----------------------\n",
    "cap = cv2.VideoCapture(CAM_INDEX)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(\"Could not open webcam. Try CAM_INDEX = 1 or check camera permissions.\")\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # mirror view\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = hands.process(rgb)\n",
    "\n",
    "    display_text = \"No hand detected\"\n",
    "    score_text = \"\"\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        hand_lms = result.multi_hand_landmarks[0]\n",
    "\n",
    "        # Draw landmarks\n",
    "        mp_draw.draw_landmarks(frame, hand_lms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Extract features\n",
    "        x63 = landmarks_to_63(hand_lms)\n",
    "\n",
    "        # Predict\n",
    "        label, score = predict_letter(x63)\n",
    "\n",
    "        recent.append(label)\n",
    "        smooth_label = majority_vote(list(recent)) or label\n",
    "\n",
    "        display_text = f\"Pred: {smooth_label}\"\n",
    "        if score is not None:\n",
    "            score_text = f\"Margin: {score:.3f}\"\n",
    "\n",
    "    # UI overlay\n",
    "    cv2.rectangle(frame, (10, 10), (320, 90), (0, 0, 0), thickness=-1)\n",
    "    cv2.putText(frame, display_text, (20, 45),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "    if score_text:\n",
    "        cv2.putText(frame, score_text, (20, 75),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "\n",
    "    cv2.imshow(\"ASL SVM - Real-time\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "hands.close()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
